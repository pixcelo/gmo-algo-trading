{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1T_open'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/python/gmo-algo-trading/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/dev/python/gmo-algo-trading/myenv/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/dev/python/gmo-algo-trading/myenv/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '1T_open'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m combined_df\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(combined_df\u001b[39m.\u001b[39mindex)\n\u001b[1;32m    167\u001b[0m \u001b[39m# Apply feature engineering and label creation directly on the original data\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m combined_df \u001b[39m=\u001b[39m feature_engineering(combined_df, \u001b[39m'\u001b[39;49m\u001b[39m1T\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    169\u001b[0m combined_df \u001b[39m=\u001b[39m create_label(combined_df, \u001b[39m'\u001b[39m\u001b[39m1T\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m model, evals_result \u001b[39m=\u001b[39m train_and_evaluate(combined_df)\n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mfeature_engineering\u001b[0;34m(df, prefix)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeature_engineering\u001b[39m(df, prefix):\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mopen\u001b[39m \u001b[39m=\u001b[39m df[\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mprefix\u001b[39m}\u001b[39;49;00m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     24\u001b[0m     high \u001b[39m=\u001b[39m df[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mprefix\u001b[39m}\u001b[39;00m\u001b[39m_high\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     25\u001b[0m     low \u001b[39m=\u001b[39m df[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mprefix\u001b[39m}\u001b[39;00m\u001b[39m_low\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/dev/python/gmo-algo-trading/myenv/lib/python3.11/site-packages/pandas/core/frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/dev/python/gmo-algo-trading/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '1T_open'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime, time, timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def load_data(file_names):\n",
    "    dfs = []\n",
    "    for file_name in file_names:\n",
    "        df = pd.read_csv(file_name, index_col=\"date\")\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "def feature_engineering(df):\n",
    "    open = df['open'].values\n",
    "    high = df['high'].values\n",
    "    low = df['low'].values\n",
    "    close = df['close'].values\n",
    "    # volume = df['_volume'].values\n",
    "    hilo = (high + low) / 2\n",
    "\n",
    "    rsi = talib.RSI(close, timeperiod=8) # default = 14\n",
    "    df['RSI'] = rsi\n",
    "    df['RSI_BB_UPPER'], df['_RSI_BB_MIDDLE'], df['_RSI_BB_LOWER'] = talib.BBANDS(rsi, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    \n",
    "    df['_RSI_ST'] = talib.RSI(close)/close\n",
    "    df['_RSI_LOG'] = log_transform_feature(talib.RSI(close))\n",
    "    df['_MACD'], _, _ = talib.MACD(close)\n",
    "    df['_MACD_ST'], _, _ = talib.MACD(close)/close\n",
    "    df['_ATR'] = talib.ATR(high, low, close)\n",
    "    df['_ADX'] = talib.ADX(high, low, close, timeperiod=14)\n",
    "    df['_ADXR'] = talib.ADXR(high, low, close, timeperiod=14)\n",
    "    \n",
    "    df['_SMA20'] = talib.SMA(close, timeperiod=20)\n",
    "    df['_SMA50'] = talib.SMA(close, timeperiod=50)\n",
    "    df['_SMA100'] = talib.SMA(close, timeperiod=100)\n",
    "    \n",
    "    df['_BB_UPPER'], df['_BB_MIDDLE'], df['_BB_LOWER'] = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    df['_BBANDS_upperband'] = (df['_BB_UPPER'] - hilo) / close\n",
    "    df['_BBANDS_middleband'] = (df['_BB_MIDDLE'] - hilo) / close\n",
    "    df['_BBANDS_lowerband'] = (df['_BB_LOWER'] - hilo) / close\n",
    "    # df['_BBANDS_bandwidth'] = (df['_BB_UPPER'] - df['_BB_LOWER']) / df['_BB_MIDDLE']\n",
    "    # df['_BBANDS_squeeze'] = (df['_BBANDS_bandwidth'] < df['_BBANDS_bandwidth'].rolling(window=20).mean()).astype(int)\n",
    "\n",
    "    df['_STOCH_K'], df['_STOCH_D'] = talib.STOCH(high, low, close)/close\n",
    "    df['_MON'] = talib.MOM(close, timeperiod=5)\n",
    "    # df['_OBV'] = talib.OBV(close, volume)\n",
    "\n",
    "    df['_support'] = df['_low'].rolling(window=20, min_periods=1).min()\n",
    "    df['_resistance'] = df['_high'].rolling(window=20, min_periods=1).max()\n",
    "\n",
    "    df = df.fillna(method='ffill')\n",
    "\n",
    "    return df\n",
    "\n",
    "def log_transform_feature(X):\n",
    "    X[X <= 0] = np.finfo(float).eps\n",
    "    return np.log(X)\n",
    "\n",
    "def price_relation(df, short_prefix, long_prefix):\n",
    "    short_close = df[f'{short_prefix}_close']\n",
    "    long_support = df[f'{long_prefix}_support']\n",
    "    long_resistance = df[f'{long_prefix}_resistance']\n",
    "    df[f'{short_prefix}_close_to_{long_prefix}_support'] = (short_close - long_support) / long_support\n",
    "    df[f'{short_prefix}_close_to_{long_prefix}_resistance'] = (short_close - long_resistance) / long_resistance\n",
    "    return df\n",
    "\n",
    "def create_label(df, prefix, lookbehind=1):\n",
    "    df['_target'] = (df['_close'] > df['_close'].shift(lookbehind)).astype(int)\n",
    "    df = df.fillna(method='ffill')\n",
    "    return df\n",
    "\n",
    "def plot_learning_curve(evals_result):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.plot(np.arange(len(evals_result['training']['binary_error'])),\n",
    "            evals_result['training']['binary_error'], label='Training')\n",
    "    ax.plot(np.arange(len(evals_result['valid_1']['binary_error'])),\n",
    "            evals_result['valid_1']['binary_error'], label='Validation')\n",
    "    ax.set_title('Learning Curve')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Binary Error')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(importances, feature_names):\n",
    "    importance = pd.DataFrame({\"Feature\": feature_names,\n",
    "                               \"Importance\": importances})\n",
    "    importance.sort_values(by=\"Importance\", ascending=False, inplace=True)\n",
    "    plt.figure(figsize=(15, 30))\n",
    "    sns.barplot(x=\"Importance\", y=\"Feature\", data=importance)\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_and_evaluate(df, n_splits=2):\n",
    "    features = df.drop('15T_target', axis=1)\n",
    "    labels = df['15T_target']\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    feature_importances = []\n",
    "\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "        y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_error',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9\n",
    "        }\n",
    "\n",
    "        evals_result = {}\n",
    "\n",
    "        model = lgb.train(\n",
    "            params=params,\n",
    "            train_set=train_data,\n",
    "            valid_sets=[train_data, test_data],\n",
    "            num_boost_round=10000,\n",
    "            callbacks=[\n",
    "                lgb.callback.early_stopping(10),\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "                lgb.callback.record_evaluation(evals_result)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = np.round(y_pred).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        plot_learning_curve(evals_result)\n",
    "        feature_importances.append(model.feature_importance())\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    print(f\"Mean accuracy: {mean_accuracy}\")\n",
    "\n",
    "    mean_importance = np.mean(feature_importances, axis=0)\n",
    "    plot_feature_importance(mean_importance, features.columns)\n",
    "\n",
    "    return model, evals_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_names = [\"combined_data.csv\"]\n",
    "    dfs = load_data(file_names)\n",
    "\n",
    "    # Assuming you have only one DataFrame in the list\n",
    "    combined_df = dfs[0]\n",
    "    combined_df.index = pd.to_datetime(combined_df.index)\n",
    "\n",
    "    # Apply feature engineering and label creation directly on the original data\n",
    "    combined_df = feature_engineering(combined_df, '1T')\n",
    "    combined_df = create_label(combined_df, '1T')\n",
    "\n",
    "    model, evals_result = train_and_evaluate(combined_df)\n",
    "\n",
    "    model_path = os.path.join(\"model\", \"simple_1m_model.pkl\")\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
